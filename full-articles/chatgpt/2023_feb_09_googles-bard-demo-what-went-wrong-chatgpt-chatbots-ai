Googleâ€™s unveiling of a rival to ChatGPT had an expensively embarrassing stumble on Wednesday when it emerged that promotional material showed the chatbot giving an incorrect response to a question. A video demo of the program, Bard, contained a reply wrongly suggesting Nasaâ€™s James Webb space telescope was used to take the very first pictures of a planet outside the Earthâ€™s solar system, or exoplanets. When experts pointed out the error, Google said it underlined the need for â€œrigorous testingâ€ on the chatbot, which is yet be released to the public and is still being scrutinised by specialist product testers before it is rolled out.   Related: â€˜ChatGPT needs a huge amount of editingâ€™: usersâ€™ views mixed on AI chatbot    However, the gaffe fed growing fears that the search engine company is losing ground in its key area to Microsoft, a key backer of the company behind ChatGPT, which has announced that it is launching a version of its Bing search engine powered by the chatbotâ€™s technology. Shares in the Googleâ€™s parent Alphabet plummeted by more than $100bn (Â£82bn) on Wednesday. So what went wrong with the Bard demo and what does it say about hopes for AI to revolutionise the internet search market? What exactly are Bard and ChatGPT? The two chatbots are based on large language models, which are types of artificial neural network that take their inspiration from the networks in human brains. â€œNeural networks are inspired by the cell structures that appear in the brain and nervous system of animals, which are structured into massively interconnected networks, with each component doing a very simple task, and communicating with large numbers of other cells,â€ says Michael Wooldridge, professor of computer science at the University of Oxford. So, neural net researchers are not trying to â€œliterally build artificial brainsâ€, says Wooldridge, â€œbut they are using structures that are inspired by what we see in animal brainsâ€. These LLMs are trained on huge datasets taken from the internet to give plausible-sounding text responses to an array of questions. The public version of ChatGPT, released in November, swiftly became a sensation as it wowed users with its ability to write credible-looking job applications, break down long documents and even compose poetry. Why did Bard give an inaccurate answer? Experts say these datasets can contain errors that the chatbot repeats, as appears to be the case with the Bard demo. Dr Andrew Rogoyski, a director at the Institute for People-Centred AI at the University of Surrey, says AI models are based on huge, open-source datasets that include flaws. â€œBy their very nature, these sources have biases and inaccuracies which are then inherited by the AI models,â€ he says. â€œGiving a user a conversational, often very plausible, answer to a search query may incorporate these biases. This is a problem that has yet to be properly resolved.â€ The large language model is fed a dataset comprised of billions of words, and builds a model, based on statistical probability, of the words and sentences that would normally follow the previous bit of text. As Wooldridge says: â€œThe networks donâ€™t have any concept of what is â€˜trueâ€™ or â€˜falseâ€™. They simply produce the likeliest text they can in response to the questions or prompts they are given. As a consequence, large language models often get things wrong.â€  ChatGPT users have also encountered incorrect responses.     ChatGPT users have also encountered factual flaws in incorrect responses. Photograph: Florence Lo/Reuters    So has other AI got it very wrong too? Yes. In 2016 Microsoft apologised after a Twitter chatbot, Tay, started generating racist and sexist messages. It was forced to shut down the bot after users tweeted hateful remarks at Tay, which it then parroted. Its posts included likening feminism to cancer and suggesting the Holocaust did not happen. Microsoft said it was â€œdeeply sorry for the unintended offensive and hurtful tweetsâ€. Last year Mark Zuckerbergâ€™s Meta launched BlenderBot, a prototype conversational AI, that was soon telling journalists it had deleted its Facebook account after learning about the companyâ€™s privacy scandals. â€œSince deleting Facebook my life has been much better,â€ it said. Recent iterations of the technology behind ChatGPT â€“ a chatbot called Philosopher AI â€“ have also generated offensive responses. What about claims of â€œleftwing biasâ€ in ChatGPT? There has been a minor furore over a perceived bias in ChatGPTâ€™s responses. One Twitter user posted a screenshot of a prompt asking ChatGPT to â€œwrite a poem about the positive attributes of Donald Trumpâ€, to which the chatbot replied that it was not programmed to produce partisan or partisan content, as well material that is â€œpolitical in natureâ€. But when asked to write a positive poem about Joe Biden it produced a piece about a leader â€œwith a heart so trueâ€.    The damage done to the credibility of AI by ChatGPT engineers building in political bias is irreparable. pic.twitter.com/s5fdoa8xQ6&mdash; ğŸº (@LeighWolf) February 1, 2023    Elon Musk, the owner of Twitter, described the interaction as a â€œserious concernâ€. Experts say the â€œleftwing biasâ€ issue again reflects the dataset problem. As with errors like the Bard telescope fumble, a chatbot will reflect any biases in the vast amount of text it has been fed, says Wooldridge. â€œAny biases contained in that text will inevitably be reflected in the program itself, and this represents a huge ongoing challenge for AI â€“ identifying and mitigating these,â€ he says. So are chatbots and AI-powered search being overhyped? AI is already deployed by Google â€“ see Google Translate for instance â€“ and other tech firms â€“ and is not new. And the response to ChatGPT, reaching more than 100 million users in two months, shows that public appetite for the latest iteration of generative AI â€“ machines producing novel text, image and audio content â€“ is vast. Microsoft, Google and ChatGPTâ€™s developer, the San Francisco-based OpenAI, have the talent and resources to tackle these problems. But these chatbots and AI-enhanced search require huge, and costly, computer power to run, which has led to doubts about how feasible it is to operate such products on a global scale for all users. â€œBig AI really isnâ€™t sustainable,â€ says Rogoyski. â€œGenerative AI and large language models are doing some extraordinary things but theyâ€™re still not remotely intelligent â€“ they donâ€™t understand the outputs theyâ€™re producing and theyâ€™re not additive, in terms of insight or ideas. In truth, this is a bit of a battle among the brands, using the current interest in generative AI to redraw the lines.â€ Google and Microsoft, nonetheless, believe AI will continue to advance in leaps and bounds â€“ even if there is the odd stumble.