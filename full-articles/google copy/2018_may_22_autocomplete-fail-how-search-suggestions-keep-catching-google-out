Sometimes, Google is just a bit too good at carrying out its stated goal to “organise the world’s information and make it universally accessible and useful”. Take search suggestions, the helpful feature that sees Google autocomplete phrases typed into its search engine. Type “How can I cook macaroni ...” and the site will add “cheese” on to the end, saving you six whole keystrokes. Wonderful! But it turns out there could be some less desirable implementations of the technology. The search-suggestion feature, and a similar feature that offers “related searches” at the bottom of the results page, could be helping to compromise the right to anonymity of complainants in UK rape and sexual assault cases. Type in the defendant’s name with a little extra information and the search engine may suggest related searches that include the victim’s name. It’s not the first time that Google, or one of its competitors in big tech, has been caught out after handing control to AI-driven, crowdsourced suggestions. • In 2016, Google was slammed for its search suggestion and related searches proposing hateful material. “Are Muslims …” offered up: “Are Muslims bad”. One search for that, and the next suggested search proposed simply: “Islam must be destroyed”. • In February this year, the company faced similar criticism: “Hitler is ...” led to “my hero”; even hyper-sanitised results, such as “Blacks are ...” – which has been manually scrubbed of almost any automatic suggestions – still offered “not oppressed” as an option. • Elsewhere in Google’s empire, YouTube’s search suggestions (the same technology, but operating on very different content) has had its own problems. For a brief period in November, “How to have ...” offered “s*x [sic] with your kids” as one option. These suggestions seem to be the modern equivalent of Googlebombs, joke search phrases created by thousands of people linking to a site with misleading text (for a long time, for instance, searching for “miserable failure” on Google led to the official biography of George W Bush). Enough people searching for one term can trick the algorithm into thinking it’s a real thing. • When the learning systems go wrong, they can sometimes go really wrong. Apple’s autocorrect changes every standalone lowercase “i” to an uppercase one – but for a while in 2017, the company instead “corrected” the letter to “A[?]”. Worse, if you accidentally texted the new strange autocorrect to someone else, their phone caught the bug, resulting in a short period where social media was essentially unreadable to large proportions of the iPhone-using world.